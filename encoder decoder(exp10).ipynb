{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FicdRMakaRn"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense,Activation, RepeatVector, Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_data = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
        "\n",
        "translation_file = open(path_to_data,\"r\", encoding='utf-8')\n",
        "raw_data = translation_file.read()\n",
        "translation_file.close()\n",
        "\n",
        "raw_data = raw_data.split('\\n')\n",
        "pairs = [sentence.split('\\t') for sentence in raw_data]\n",
        "pairs = pairs[1000:20000]\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "  # Lower case the sentence\n",
        "  lower_case_sent = sentence.lower() # Strip punctuation\n",
        "  string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "  clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
        "  return clean_sentence\n",
        "\n",
        "def tokenize(sentences): # Create tokenizer\n",
        "  text_tokenizer = Tokenizer() # Fit texts\n",
        "  text_tokenizer.fit_on_texts(sentences)\n",
        "  return text_tokenizer.texts_to_sequences(sentences), text_tokenizer\n",
        "\n",
        "english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
        "spanish_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
        "\n",
        "# Tokenize words\n",
        "spa_text_tokenized, spa_text_tokenizer = tokenize(spanish_sentences)\n",
        "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
        "\n",
        "print('Maximum length spanish sentence: {}'.format(len(max(spa_text_tokenized,key=len))))\n",
        "print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))\n",
        "\n",
        "# Check language length\n",
        "spanish_vocab = len(spa_text_tokenizer.word_index) + 1\n",
        "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
        "print(\"Spanish vocabulary is of {} unique words\".format(spanish_vocab))\n",
        "print(\"English vocabulary is of {} unique words\".format(english_vocab))\n",
        "\n",
        "max_spanish_len = int(len(max(spa_text_tokenized,key=len)))\n",
        "max_english_len = int(len(max(eng_text_tokenized,key=len)))\n",
        "spa_pad_sentence = pad_sequences(spa_text_tokenized, max_spanish_len, padding = \"post\")\n",
        "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding = \"post\")\n",
        "\n",
        "spa_pad_sentence = spa_pad_sentence.reshape(*spa_pad_sentence.shape, 1)\n",
        "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)\n",
        "\n",
        "input_sequence = Input(shape=(max_spanish_len,))\n",
        "embedding = Embedding(input_dim=spanish_vocab, output_dim=128,)(input_sequence)\n",
        "encoder = LSTM(64, return_sequences=False)(embedding)\n",
        "r_vec = RepeatVector(max_english_len)(encoder)\n",
        "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n",
        "logits = TimeDistributed(Dense(english_vocab))(decoder)\n",
        "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
        "enc_dec_model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(1e-3), metrics=['accuracy'])\n",
        "enc_dec_model.summary()\n",
        "\n",
        "enc_dec_model.fit(spa_pad_sentence, eng_pad_sentence, epochs=50)\n",
        "\n",
        "def logits_to_sentence(logits, tokenizer):\n",
        "  index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<empty>'\n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "indexes = [1, 17]\n",
        "for index in indexes:\n",
        "  print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
        "  print(\"The spanish sentence is: {}\".format(spanish_sentences[index]))\n",
        "  print('The predicted sentence is :')\n",
        "  print(logits_to_sentence(enc_dec_model.predict(spa_pad_sentence[index:index+1])[0], eng_text_tokenizer))\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOup53aqn0dWaTT1CEbYUmd",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
